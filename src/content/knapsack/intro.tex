The study of combinatorial optimization problems with a submodular objective has attracted much attention in the last decade. 
A set function $f:2^\mathcal{N} \to \mathbb{R}_+$ over a ground set $\mathcal{N}$ is called \emph{submodular} if it has the \emph{diminishing returns} property:
$f(A \cup \{a\}) - f(A) \geq f(B \cup \{a\}) - f(B)$ for every $A \subseteq B \subseteq \mathcal{N}$ and $a \in \mathcal{N} \setminus B$\footnote{
    An equivalent definition is: $f(A) + f(B) \geq f(A \cup B) + f(A \cup B)$ for every $A,B \in \mathcal{N}$.
}
Submodular functions capture the principle of economy of scale, prevalent in bothe theory and real world applications.
Thus, it is no surprise that combinatorial optimization problems with a submodular objective arise in numerous disciplines, e.g., machine learning and data mining~\cite{bach2013learning,bordeaux2014tractability}, algorithmic game theory and social networks~\cite{dughmi2009revenue,hartline2008optimal,he2015stability,kempe2003maximizing,schulz2013approximating}, and economics~\cite{ahmed2011maximizing}.
Additionally, many classical problems in combinatorial optimization are in fact submodular in nature, e.g., maximum cut and maximum directed cut~\cite{goemans1995improved,}


In the \emph{Budgeted Maximum Cover} problem we are given a ground set of elements
$X = \{x_1, \dots, x_n\}$ and a collection of subsets over $X$,
$\mathcal{S} = \{S_1, \dots, S_m\}$.
A cost function, $c:\mathcal{S} \to \mathbb{R}_+$ assigns cost to each set
and a weight function $w:X \to \mathbb{R}_+$ assigns weight to each element.
The goal is to find a collection of sets $\mathcal{S'} \subseteq \mathcal{S}$ 
such that the total cost of the sets in the collection does not exceed a given budget, 
$\beta$, and the
total weight of the elements covered by $\mathcal{S'}$ is maximal.
Khuller et. al \cite{khuller1999budgeted}
gave a $(1-e^{-1})$-approximation algorithm for this problem and showed that this
is the best possible unless P = NP.

A set function $f$ is \emph{submodular} if $f(A \cap B) + f(A \cup B) \leq f(A) + f(B)$
for every two sets in the domain of the function. A set function, $f$, 
is \emph{monotone} if
$f(A) \leq f(B)$ for every two sets, $A \subseteq B$, in the domain of the function.
Given a set of elements $U = \{e_1, \dots, e_n\}$, a cost function,
$c:U \to \mathbb{R}_+$ and a monotone, submodular function, $f:2^U \to \mathbb{R}_+$
the goal in the 
\emph{Submodular Function Under Knapsack Constraint Maximization} 
problem is to find a subset of $U$ with total cost that does not exceed
a given budget, $\beta$, that maximizes $f$.
This problem generalized the \emph{Budgeted Maximum Cover} problem.
Submodular maximization has applications in machine learning such as in document summarization~\cite{lin2010multi}.

Sviridenko \cite{sviridenko2004note} showed that the same algorithm presented by
Khuller et. al can be used to maximize a general monotone submodular function
with the same guarantee.
This algorithm requires $O(n^5)$ calls to the value oracle.
This algorithm might be impractical for real world applications~\cite{lin2010multi}.

A faster algorithm that runs in $O(n^2)$ time and achieves a $1 - e^{-1/2}$ approximation ratio
was also presented in \cite{khuller1999budgeted}.
It was shown by Krause and Guestrin \cite{krause2005note} that the same algorithm
gives the same guarantee for a general monotone submodular function and requires only
$O(n^2)$ calls to the value oracle.
In the above papers (and other papers as well), however,
there is a logical flaw in the analysis of this algorithm.
In this paper we prove this claim.
We also consider a modification of this algorithm with the same running time and show that it
achieves a better approximation ratio.

Badanidiyuru and VondrÂ´ak developed a $1 - \frac{1}{e} - \epsilon$-approximation 
algorithms that runs in 
$O(n^2(\frac{1}{\epsilon}\log n)^\text{poly}(\frac{1}{\epsilon}))$ time 
\cite{badanidiyuru2014fast}.
Ene and Nguyen developed an even faster algorithm that runs in 
$\frac{1}{\epsilon}^{O(1/\epsilon^4)}n \log^2 n$ time
\cite{Alina2017}.
These algorithms, however, 
as the authors of the latter one mentioned, are impractical.
For example, the running time of the later algorithm for $\epsilon = 2^{-2}$ is
$2^{2O(2^{8})}n\log^2n$ achieving approximation ratio of $\approx 0.38$.
Here we develop a $(1 - e^{-2/3}) \approx 0.48$-approximation algorithm with the same
asymptotic running time as the simple greedy algorithm and with reasonable constants.
