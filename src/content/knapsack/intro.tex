The study of combinatorial optimization problems with a submodular objective has attracted much attention in the last decade. 
A set function $f:2^\mathcal{N} \to \mathbb{R}_+$ over a ground set $\mathcal{N}$ is called \emph{submodular} if it has the \emph{diminishing returns} property:
$f(A \cup \{a\}) - f(A) \geq f(B \cup \{a\}) - f(B)$ for every $A \subseteq B \subseteq \mathcal{N}$ and $a \in \mathcal{N} \setminus B$\footnote{
    An equivalent definition is: $f(A) + f(B) \geq f(A \cup B) + f(A \cup B)$ for every $A,B \in \mathcal{N}$.
}
Submodular functions capture the principle of economy of scale, prevalent in bothe theory and real world applications.
Thus, it is no surprise that combinatorial optimization problems with a submodular objective arise in numerous disciplines, e.g., machine learning and data mining~\cite{bach2013learning,bordeaux2014tractability}, algorithmic game theory and social networks~\cite{dughmi2009revenue,hartline2008optimal,he2015stability,kempe2003maximizing,schulz2013approximating}, and economics~\cite{ahmed2011maximizing}.
Additionally, many classical problems in combinatorial optimization are in fact submodular in nature, e.g., maximum cut and maximum directed cut~\cite{goemans1995improved,Halperin:2001:CAA:365411.365412,Hastad:2001:OIR:502090.502098,Karp1972,Khot05optimalinapproximability}, maximum coverage~\cite{Feige:1998:TLN:285055.285059,KHULLER199939}, generalized assignment problem~\cite{Chekuri06apolynomial,Cohen06anefficient,Feige2006ApproximationAF,Fleischer:2006:TAA:1109557.1109624}, maximum bisection~\cite{DBLP:journals/talg/AustrinBG16,DBLP:journals/algorithmica/FriezeJ97}, and facility location~\cite{DBLP:journals/dam/AgeevS99,CORNUEJOLS1977163,doi:10.1287/mnsc.23.8.789}.

In this paper we consider the problems of maximizing a monotone\footnote{
    $f$ is monotone if $f(S) \leq f(T)$ for every $S \subseteq T \subseteq \mathcal{N}$.
} submodular function under a knapsack constraint.
In this problem we are given a set of elements
$X = \{x_1, \dots, x_n\}$, a submodular function $f:2^X \to \mathbb{R}_+$, and a cost function, $c:X \to \mathbb{R}_+$.
The goal is to find a subset of elements that maximizes $f$ such that the total cost of the elements in the subset does not exceed a given budget, 
$\beta$.

Maximizing a monotone submodular function under a knapsack constraint generalized the budgeted maximum coverage problem~\cite{khuller1999budgeted} and has applications such as document summarization~\cite{lin2010multi} and maximizing entropy in discrete graphical models~\cite{krause2005note}.


\paragraph*{Previous Work}
Nemhauser et al. considered the problem of maximizing a monotone submodular function under cardinality constant \cite{Nemhauser1978}.
They proved that the greedy algorithm (one that iteratively construct a solution by selecting each time the best element) gives $(1 - e^{-1})$ approximation.
Khuller et. al considered the Budgeted Maximum Coverage problem\cite{khuller1999budgeted}.
A coverage objective is a special case of submodular objective.
They gave a $(1-e^{-1})$-approximation algorithm for this problem and showed that this is the best possible unless P = NP.
Sviridenko \cite{sviridenko2004note} showed that the same algorithm presented by
Khuller et. al can be used to maximize a general monotone submodular function
with the same guarantee.
This algorithm requires $O(n^5)$ calls to the value oracle and might be impractical for real world applications~\cite{lin2010multi}.

A faster algorithm that runs in $O(n^2)$ time and achieves a $1 - e^{-1/2}$ approximation ratio was also presented in \cite{khuller1999budgeted}.
It was shown by Krause and Guestrin \cite{krause2005note} that the same algorithm
gives the same guarantee for a general monotone submodular function and requires only $O(n^2)$ calls to the value oracle.

Badanidiyuru and VondrÂ´ak developed a $1 - \frac{1}{e} - \epsilon$-approximation 
algorithms that runs in 
$O(n^2(\frac{1}{\epsilon}\log n)^\text{poly}(\frac{1}{\epsilon}))$ time 
\cite{badanidiyuru2014fast}.
Ene and Nguyen developed an even faster algorithm that runs in $\frac{1}{\epsilon}^{O(1/\epsilon^4)}n \log^2 n$ time \cite{Alina2017}.
These algorithms, however, as mentioned by the authors, are impractical.
For example, the running time of the latter algorithm for $\epsilon = 2^{-2}$ is
$2^{2O(2^{8})}n\log^2n$ achieving approximation ratio of $\approx 0.38$.

% In the above papers (and other papers as well), however,
% there is a logical flaw in the analysis of this algorithm.
% In this paper we prove this claim.
% We also consider a modification of this algorithm with the same running time and show that it
% achieves a better approximation ratio.

% asymptotic running time as the simple greedy algorithm and with reasonable constants.

